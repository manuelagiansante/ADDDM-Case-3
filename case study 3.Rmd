---
title: "Case Study 3"
author: "Manuela Giansante & Lucia Camenisch"
date: "2023-04-15"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=FALSE, message=FALSE}
library(data.table)      # data.table type
library(rcompanion)      # plothistogram()
library(tidyr)           # gather()
library(kableExtra)      # kables for html document
library(naniar)          # vis_miss()
library(corrplot)        # corrplot()
library(caret)           # findCorrelation()
library(REdaS)           # KMOS(), bart_spher()
library(psych)           # KMO(), fa(), principal()
```


## Galeries Lafayette
We load the data files.

```{r}
descriptions <- fread("Variables and Labels_Galeries Lafayette.csv")
df <- read.csv("Case Study III_Structural Equation Modeling.csv")

```

We investigate missing values in the data:

```{r}
mean(df==999)# percentage of missing data
df1<- data.frame(sapply(df,function(x) ifelse((x==999),NA,as.numeric(x))))

# Replace NA with column mean
for (i in 1:ncol(df1)) {
  col_mean <- mean(df1[,i], na.rm = TRUE)
  df1[is.na(df1[,i]), i] <- col_mean
}

df2<- df1[,c(1:22)]

```
We have almost 3% of missing data, deleting it would mean we would work with a little more than half the data we have now. We therefore replace the 999 values with the column mean.

### Exploratory and Confirmatory Factor Analysis
#### Exploratory

We first plot the variables so to have a sense of how client perception is skewed.

```{r}
# Plot the variables
par(mfrow = c(3, 3))
for (col in c(1:ncol(df2))){ 
  plotNormalHistogram(df2[col],main = paste("Frequency Distribution of Im", col))
  }
```

For most questions the distribution is skewed to the left, answers leaned towards the "does apply completely" direction.
We run a correlation matrix so to see if we can individuate some grouping in our variables.

```{r}
raqMatrix <- cor(df2)
corrplot(raqMatrix, order = 'hclust', tl.cex = 0.8, addrect = 10)
```

We check the description files to see if the clusters are thematically coherent:
- Im8, Im10, Im14: They all regard food and the quality of food (French cuisine, gourmet food);
- Im6, Im7: French Lifestyle;
- Im12, Im13: Luxury and designer brands;
- Im20,Im21, Im22: They are about the feeling of the shopping experience, so whether the customer feels at ease;
- Im16, Im19: They are about the perception of professionalism;
- Im15, Im1, Im2: They cover the brand/ product assortment;
- Im18, Im17: Describe how on trend the Galeries are perceived to be;
- Im5, Im3, Im4: Measure how the client perceives the decorations and arrangement of the shopping areas.

Variables Im9, Im11 seem to stand on their own in terms of correlations, in this order the respective questions concern French Fashion and French Cosmetics. Especially the latter one seems to be isolated in terms of correlation and meaning.


```{r}
# Kaiser factor adequacy
KMO(df2)
```

We computed the Overall measure sampling adequacy is 0.88 the variables seem adequate enough for factor analysis. The lowest MSA is achieved by Im2, but still it is a quite high value so we should not worry too much. Im9 and Im11 both have high MSA which gien that they stood on their own in the correlation matrix it is reassuring.

```{r}
anti_mat_diag = data.frame("Question" = 1:22,
                           "UniqueVar" = diag(KMO(df2)$ImCov))

ggplot(data = anti_mat_diag, aes(x = Question, y = UniqueVar)) +
  geom_col() +
  theme_classic() +
  scale_x_continuous(breaks = seq(1, 22, 1)) +
  scale_y_continuous(limits = c(0,1)) +
  labs(title = "Diagonal values of anti-image correlation matrix",
       y = "Proportion of unique variance")
```

We plot a bar chart of the unique variances, there not seem to be very big outliers, yet Im9 and Im11 do stand out, we saw how those questions were unique in terms of the matter addressed, so definitely the questionnaire could be improved by going deeper into the french fashion and french cosmetics subjects.
This is also considering that marketing wise these 2 are very high selling points especially for tourists who might come and look for something more "peculiar" and identifiable as being from Paris.

```{r}
# Barlett's Test of Sphericity
bart_spher(df2)
```

We are testing whether the sample of data we have stems from a population of uncorrelated variables, meaning the correlation matrix is an identity matrix (diagonal of 1s).The test is significant, the correlation matrix is not an identity matrix, there is correlation among the variables.

#### Principal Axis Factoring

We use a starting number of factors 15 to avoid errors from the system but still get a a fair number of runs.
For clarity we describe the used criteria, so the have a more readable plot:
- fit, which measures how well the factor model reproduces the correlation matrix;
- objective, which is the value of the function that is minimized by a maximum likelihood procedure;
- rms, which is the sum of the squared off-diagonal residuals divided by the degrees of freedom;
- crms, which is the RMS adjusted for degrees of freedom;
- TLI, which is the Tucker Lewis Index of factoring reliability;
- BIC, which is the Bayesian information criterion.

```{r}
# we initiate and empty dataframe which will record our criteria values
fit_df <- matrix(nrow = 15, ncol = 6)
colnames(fit_df) <- c("fit", "objective", "rms", "crms", "TLI", "BIC")
fit_df <- as.data.frame(fit_df)

# we compute the factor analysis for nfactors 
for (i in 1:15) {
  FA <- fa(df2, nfactors = i, rotate = "varimax", fm = "pa", SMC = FALSE)
  fit_df[i, 1] <- FA$fit
  fit_df[i, 2] <- FA$objective
  fit_df[i, 3] <- FA$rms
  fit_df[i, 4] <- FA$crms
  fit_df[i, 5] <- FA$TLI
  fit_df[i, 6] <- FA$BIC
}

```

```{r}
fit_df %>% gather() %>%                 
  ggplot(aes(x = rep(1:15, ncol(fit_df)), y = value)) +
  facet_wrap(~ key, scales = "free") +
  geom_point() +
  theme_classic() +
  scale_x_continuous(breaks = seq(1, 15, 1)) +
  geom_vline(xintercept = 8, linetype = "dashed") +
  geom_vline(xintercept = 9, linetype = "dashed") +
  labs(title = "Various criteria for Principal Axis Factoring",
       x = "Number of factors",
       y = "")
```

It seems that the number of factors we want to try and fit is 8 or 9.

```{r, message=FALSE} 
FA9 <- fa(df2, nfactors = 9, rotate = "varimax", fm = "pa")
FA8 <- fa(df2, nfactors = 8, rotate = "varimax", fm = "pa")

FA9_119 <- fa(df2[, -c(9,11)], nfactors = 9, rotate = "varimax", fm = "pa")
FA8_119 <- fa(df2[, -c(9,11)], nfactors = 8, rotate = "varimax", fm = "pa")
```


```{r}
par(mfrow = c(2,2))
corrplot(t(FA8$loadings),
         tl.cex = 0.7,
         title = "PAF with 8 factors \n Loadings",
         mar = c(0, 1, 3, 0))
corrplot(t(FA8_119$loadings),
         tl.cex = 0.7,
         title = "PAF with 8 factors excluding im9 and im11 \n Loadings",
         mar = c(0, 1, 3, 0))
corrplot(t(FA9$loadings),
         tl.cex = 0.7,
         title = "PAF with 9 factors \n Loadings",
         mar = c(0, 1, 3, 0))
corrplot(t(FA9_119$loadings),
         tl.cex = 0.7,
         title = "PAF with 9 factors excluding im9 and im11 \n Loadings",
         mar = c(0, 1, 3, 0))
```

We plot the loading of all the models we estimated, we notice that the comprehensive models perform badly on the loadings for Im9 on factor 2, Im11 is not as low on factor 5 but it is not brilliant.

We investigate what teh loadings look like in case we only take out the Im9:

```{r}
FA9_9 <- fa(df2[, -c(9)], nfactors = 9, fm = "pa")
FA8_9 <- fa(df2[, -c(9)], nfactors = 8, rotate = "varimax", fm = "pa")

par(mfrow = c(1,2))
corrplot(t(FA8_9$loadings),
         tl.cex = 0.7,
         title = "PAF with 9 factors excluding im9 \n Loadings",
         mar = c(0, 1, 3, 0))
corrplot(t(FA9_9$loadings),
         tl.cex = 0.7,
         title = "PAF with 9 factors excluding im9 \n Loadings",
         mar = c(0, 1, 3, 0))
```

The performance of Im11 in terms of loadings on factors does not improve, which conceptually makes sense because the 2 variables describe something quite different.
We get rid of the full models and print the loadings for remaining ones:

```{r}
print(FA8_9$loadings, cutoff = 0.3, sort = TRUE)
print(FA8_119$loadings, cutoff = 0.3, sort = TRUE)
print(FA9_9$loadings, cutoff = 0.3, sort = TRUE)
print(FA9_119$loadings, cutoff = 0.3, sort = TRUE)
```

The model with the highest cumulative variance is the FA9_911, but if we observe more carefully we notice that no variables are loading on factor PA9. Therefore, the next best option is model FA8_911.
In this model Im15 and Im19 (very close to cut off) are partially loading on factors PA4 and PA1, we check again their descriptions to understand why that might be:
In the correlation matrix they grouped, respectively, with brand assortment and perception of professionalism.
PA4 in our case describes the client perception of arrangement of the Galeries, variable 9 is about the organization which logically can be connected.
PA1 and variable 15 grouped together in the correlation matrix already which is encouraging, they are about brand assortment.
```{r}
descriptions[c(15,19),2]
```

We check communalities and the Scree plot to get further insight on how to handle Im9 and Im11:
 
```{r}
ggplot(mapping = aes(x = 1:length(FA9_119$values),
                     y = FA9_119$values,
                     color = (FA9_119$values >= 1))) +
  geom_point() +
  geom_hline(yintercept = 1, linetype = "dashed") +
  theme_classic() +
  labs(title = "Scree plot of FA9_119",
       x = "Factor Number",
       y = "Eigenvalue",
       color = "Eigenvalue >= 1")
```

The scree plot for the model FA9_119 suggests, by the elbow rule to cut off at factor number 8, which is coherent, we saw how no variable would load on the ninth factor.Meaning that PA9 explains no variation in the variables, it brings no further aid in explaining the structure of our data.

```{r}
# Compare the excluding model with the comprehensive one
sort(FA9_119$communality)
sort(FA9$communality)
```

Im9 and Im11 are respectively 0.406 and 0.625 which is quite low.
```{r}
sort(FA8_119$communality)
```

In the 8 factor model some variables gain, some lose in terms of communality values, so how correlated they are with other variables in the factor analysis.
As our final model we select FA8_911, and proceed with confirmatory analysis.

#### Confirmatory

The "outcomes" for our models will be Repurchase intention and Cocreation Intention.
The "mediators" will be "customer Satisfacton" and "Affective Commitment"
```{r}

```




