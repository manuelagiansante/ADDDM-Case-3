---
title: "Case Study 3"
author: "Manuela Giansante & Lucia Camenisch"
date: "2023-04-15"
output:
  html_document:
    toc: true # creating a table of contents (toc)
    toc_float:
      collapsed: false # toc does not collapse and is shown as a sidebar (toc_float)
    number_sections: true # document sections are numbered
    theme: cosmo
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.align = "center")
```

```{r}
library(data.table)      # data.table type
library(rcompanion)      # plothistogram()
library(tidyr)           # gather()
library(kableExtra)      # kables for html document
library(naniar)          # vis_miss()
library(corrplot)        # corrplot()
library(caret)           # findCorrelation()
library(REdaS)           # KMOS(), bart_spher()
library(psych)           # KMO(), fa(), principal()
library(lavaan) 
library(semPlot)         # for visualization
library(psy)
library(lavaanPlot)
```

# Data Exploration and Preprocessing

We load the data files.

```{r}
descriptions <- fread("Variables and Labels_Galeries Lafayette.csv")
df <- read.csv("Case Study III_Structural Equation Modeling.csv")
```

We investigate missing values in the data, we know that missing values
have been coded as "999" in the dataset:

```{r}
# replacing 999 by NA values
df1 <- df
df1[df1 == 999] <- NA

# separating dataframe only with the questionnaire data
df2 <- df1[,c(1:22)]

# proportion of rows containing NA values
1 - nrow(na.omit(df2)) / nrow(df2)

# deleting rows with NAs
df2 <- na.omit(df2)
nrow(df2)
```

As it was instructed, we delete the rows containing missing values. This represents 30% of our dataset, which is quite a high amount. We are left with 385 rows

We first plot the variables to have a sense of how client perception is
skewed.

```{r}
# Plot the variables
par(mfrow = c(3, 3))
for (col in c(1:ncol(df2))){ 
  plotNormalHistogram(df2[col],main = paste("Frequency Distribution of Im", col))
}
rm(col)
```

For most questions the distribution is skewed to the left, answers
leaned towards the "does apply completely" direction. We observe that
for question number 7 asking whether Galeries Lafayette Berlin embody
French Savoir-vivre for the client, none has answered "not at all",
because the range of the distribution starts from 2. Question 10 about
whether the Galeries represent gourmet food has also received no
completely negative answers.

We run a correlation matrix so to see if we can individuate some
groupings in our variables.

```{r}
raqMatrix <- cor(df2)
corrplot(raqMatrix, order = 'hclust', tl.cex = 0.8, addrect = 10)
```

The clusters that form just by observing correlation between variable
are a first suggestions of what kind of factors we can identify. We
check the description files to see if the clusters are thematically
coherent:

- `Im8`, `Im10`, `Im14`: They all regard food and the quality of food
    (French cuisine, gourmet food);

- `Im6`, `Im7`: French Lifestyle;

- `Im12`, `Im13`: Luxury goods and brands;

- `Im20`, `Im21`, `Im22`: They are about the feeling of the shopping
    experience, so whether the customer feels at ease;

- `Im16`, `Im19`: They are about the perception of professionalism;

- `Im15`, `Im1`, `Im2`: They cover the brand/product assortment;

- `Im18`, `Im17`: Describe how on trend the Galeries are perceived to
    be;

- `Im5`, `Im3`, `Im4`: Measure how the client perceives the
    decorations and arrangement of the shopping areas.

Variables `Im9` and `Im11` seem to stand on their own in terms of
correlations, they concerns French fashion and cosmetics respectively. `Im9` in particular has no strong correlation with any other variable.

We display the highest correlation any variable has with variables except itself.

```{r}
var_cors <- apply(upper.tri(raqMatrix)*raqMatrix + lower.tri(raqMatrix)*raqMatrix,2,max)
ggplot(mapping = aes(x = reorder(names(var_cors), var_cors), y = var_cors)) +
  geom_col() +
  theme_classic() +
  scale_y_continuous(limits = c(0,1), breaks = seq(0, 1, 0.1)) +
  labs(x = "Variable",
       y = "Highest correlation",
       title = "Highest correlation of variables with other variables")
rm(var_cors)
```

Clearly, there are two considerable jumps between `Im9`, `Im11` and the rest of variables. We need to look out for these two variables and assess whether they should be included in our model or not.

We compute the Kaiser-Meyer-Olkin test.

```{r}
# Kaiser factor adequacy
KMO(df2)
```

The overall Measure of Sampling Adequacy is 0.88. The variables seem
adequate enough for factor analysis. According to Kaiser, a MSA in the
.80s is "meritorious". The lowest MSA is achieved by `Im2`, but still it
is a quite high value so we should not worry too much.Moreover, the
correlation grouping it formed with `Im15` and
I\``m1 had a strong common theme.`Im9`and`Im11\` both have high MSA
despite them standing on their own in the correlation matrix, so it is
reassuring.

We plot a bar chart of the unique variance of each variable.

```{r}
anti_mat_diag = data.frame("Question" = 1:22,
                           "UniqueVar" = diag(KMO(df2)$ImCov))

ggplot(data = anti_mat_diag, aes(x = Question, y = UniqueVar)) +
  geom_col() +
  theme_classic() +
  scale_x_continuous(breaks = seq(1, 22, 1)) +
  scale_y_continuous(limits = c(0,1)) +
  labs(title = "Diagonal values of anti-image correlation matrix",
       y = "Proportion of unique variance")
```

There doesn't seem to be very big outliers, but `Im9` and `Im11` do
stand out. We saw how those questions were unique in terms of the matter
addressed, so the questionnaire definitely could be improved by going
deeper into the french fashion and french cosmetics subjects. This is
also important considering that marketing-wise these two subjects are
very high selling points especially for tourists who might come and look
for something more authentic and identifiable as being from Paris.

We use Bartlett's test of sphericity.

```{r}
# Barlett's Test of Sphericity
bart_spher(df2)
```

We are testing whether the sample of data we have stems from a
population of uncorrelated variables, meaning the correlation matrix is
an identity matrix (diagonal of 1s). This is the null hypothesis. Since
we get a very small $p$-value under 0.05, the null hypothesis can be
rejected, meaning that the correlation matrix is not an identity matrix
and there are strong correlations among the variables.

# Exploratory Factor Analysis with Principal Axis Factoring


We explore models with 1 to 10 factors, as adding more factors results in an error. And plot some key measures which asses how well the model represents the underlying structure of our dataset.
For clarity, we describe the criteria used below, to facilitaty comprehension:

-   fit, which measures how well the factor model reproduces the
    correlation matrix;

-   objective, which is the value of the function that is minimized by a
    maximum likelihood procedure;

-   crms, which is the sum of the squared off-diagonal residuals divided
    by the degrees of freedom adjusted for degrees of freedom;

-   RMSEA, which is the root mean squared error of approximation;

-   TLI, which is the Tucker Lewis Index of factoring reliability;

-   BIC, which is the Bayesian information criterion.

```{r}
# we initiate and empty dataframe which will record our criteria values
fit_df <- matrix(nrow = 10, ncol = 6)
colnames(fit_df) <- c("fit", "objective", "RMSEA", "crms", "TLI", "BIC")
fit_df <- as.data.frame(fit_df)

# we compute the factor analysis for nfactors 
for (i in 1:10) {
  FA <- fa(df2, nfactors = i, rotate = "varimax", fm = "pa")
  fit_df[i, 1] <- FA$fit
  fit_df[i, 2] <- FA$objective
  fit_df[i, 3] <- FA$RMSEA[1]
  fit_df[i, 4] <- FA$crms
  fit_df[i, 5] <- FA$TLI
  fit_df[i, 6] <- FA$BIC
}
```

```{r}
fit_df %>% gather() %>%                 
  ggplot(aes(x = rep(1:10, ncol(fit_df)), y = value)) +
  facet_wrap(~ key, scales = "free") +
  geom_point() +
  theme_classic() +
  scale_x_continuous(breaks = seq(1, 10, 1)) +
  geom_vline(xintercept = 8, linetype = "dashed") +
  geom_vline(xintercept = 9, linetype = "dashed") +
  labs(title = "Various criteria for Principal Axis Factoring",
       x = "Number of factors",
       y = "")

rm(FA, fit_df, i)
```

It seems that the number of factors we want to try and fit is 8 or 9.

## Full models with 8 or 9 factors

We start fitting our first models, of course selecting our final model
will be a process of trial and error.

We try models with 8 or 9 factors and with or without `Im9` and `Im11`.

```{r, message=FALSE}
# including all images
FA9 <- fa(df2, nfactors = 9, rotate = "varimax", fm = "pa")
FA8 <- fa(df2, nfactors = 8, rotate = "varimax", fm = "pa")

# without images 9 an 11
FA9_119 <- fa(df2[, -c(9, 11)], nfactors = 9, rotate = "varimax", fm = "pa")
FA8_119 <- fa(df2[, -c(9, 11)], nfactors = 8, rotate = "varimax", fm = "pa")
```

We print the loadings of these models as heatmaps:

```{r}
par(mfrow = c(2,2))
corrplot(t(FA8$loadings),
         tl.cex = 0.7,
         title = "PAF with 8 factors \n Loadings",
         mar = c(0, 1, 3, 0))
corrplot(t(FA8_119$loadings),
         tl.cex = 0.7,
         title = "PAF with 8 factors excluding im9 and im11 \n Loadings",
         mar = c(0, 1, 3, 0))
corrplot(t(FA9$loadings),
         tl.cex = 0.7,
         title = "PAF with 9 factors \n Loadings",
         mar = c(0, 1, 3, 0))
corrplot(t(FA9_119$loadings),
         tl.cex = 0.7,
         title = "PAF with 9 factors excluding im9 and im11 \n Loadings",
         mar = c(0, 1, 3, 0))
```

When plotting the loadings of all the models we estimated, we notice that for the two full models, `Im9` has no strong loadings on any factor. `Im11` seems to be loaded on factor 4, which is the construct related to luxury. As `Im11` mentions high quality cosmetics, this kind of products can be related to the luxury milieu.

When looking at the factors, we see that both models with 9 factors have no strong loadings on factor 9, which indicates that models with 8 are probably more suited for our data.

We also notice that `Im8`, `Im15` and `Im19` seem to have difficulties, they are partially loaded onto multiple factors or not loaded very strongly on one factor. We have to investigate these questions further. To do so, we look at their descriptions.

```{r}
descriptions[c(8,15,19),]
```


- `Im8` assesses the "Expertise in French traditional cuisine". Thus, it loads both on the construct we called *French lifestyle* and onto *Food*.
- `Im15` assesses the "Professional selection of brands". Thus, it loads on both *Product assortment* and *Professionalism*.
- `Im19` assesses whether Galeries Lafayette are a "Professional organisation". This is under the *Professionalism* construct as well, but its loading is overall weaker.


## Models without Image 9 (8 and 9 factors)

We investigate what the loadings look like in case we only take out the
`Im9`:

```{r}
FA8_9 <- fa(df2[, -c(9)], nfactors = 8, rotate = "varimax", fm = "pa")
FA9_9 <- fa(df2[, -c(9)], nfactors = 9, rotate = "varimax", fm = "pa")

par(mfrow = c(2,1))
corrplot(t(FA8_9$loadings),
         tl.cex = 0.7,
         title = "Loadings of PAF with 8 factors excluding im9",
         mar = c(0, 1, 3, 0))
corrplot(t(FA9_9$loadings),
         tl.cex = 0.7,
         title = "Loadings of PAF with 9 factors excluding im9",
         mar = c(0, 1, 3, 0))
par(mfrow = c(1,1))
```

The performance of `Im11` in terms of loadings on factors does not
improve, which conceptually makes sense because the 2 variables describe
something quite different.

Again, the model with 9 factors does not have any strong loading on its last factor. Therefore, we decide to continue the analysis for now by comparing only with our models which have 8 factors and which exclude at least `Im9`, i.e. `FA8_9` and `FA8_119`. All other models will be dropped from further analysis.

```{r}
rm(FA8, FA9, FA9_9, FA9_119)
```

We display the loadings of our two remaining models numerically, with a cutoff at 0.3.

```{r}
print(FA8_9$loadings, cutoff = 0.3, sort = TRUE)
print(FA8_119$loadings, cutoff = 0.3, sort = TRUE)
```

The two models are quite similar, but we can see that the loading associated to variable `Im11` inside `FA8_9` is under 0.6. Moreover, the portion of variance explained by the model is 0.771 for `FA8_9` and 0.787 for `FA8_119`. Thus, `FA8_119` seems to be a better option. 

The three variables mentioned before are still loading onto multiple factors:
- `Im8` is loading onto Food and French Lifestyle constructs like before;
- `Im15` is loading onto Product Assortment and Professionalism like before;
- `Im19` is loading onto Store Arrangement and Professionalism;
- `Im7` which is part of the French Lifestyle construct, is loading onto the Store Arrangement construct as well, but the difference in magnitude of the loadings is important, unlike with the other three variables above. It maintains a strong attachment to its designated factor.

We look at the description of question `Im7`.

```{r}
descriptions[7,]
```
There does not seem to be a particular logical connection between French Savoir-vivre and the the Store Arrangement construct. However, as we have mentioned this is not a huge concern, as `Im7` is strongly anchored to the French Lifestyle construct. Regarding `Im8`, `Im15` and `Im19`, we will have to test models including or excluding them.


## Further investigation on `FA8_119`

We display the scree plot for `FA8_119`.

```{r}
# Scree Plot FA8_119
ggplot(mapping = aes(x = 1:length(FA8_119$values),
                     y = FA8_119$values,
                     color = (FA8_119$values >= 1))) +
  geom_point() +
  geom_hline(yintercept = 1, linetype = "dashed") +
  theme_classic() +
  labs(title = "Scree plot of FA8_119",
       x = "Factor Number",
       y = "Eigenvalue",
       color = "Eigenvalue >= 1")
```

This plot serves the purpose of
determining the number of factors to retain in the factor axis analysis.
Factors' eigenvalues are ordered in a descending order. This method is
quite subjective but it might provide provide further insight. The elbow rule suggests to cut off the number of factors at 8, which is coherent with what we have seen. However, the Kaiser-Guttman criterion, which indicates to select only factors with eigenvalues greater than 1, would suggest here a model with only five factors. Give the natural groupings we have considered above, this does not seem like an appropriate description of our data's structure. However, this model will still be explored in the following section.


## Model with 5 factors

We run a model with only 5 factors and without `Im9` and `Im11`, as the scree plot considered above which suggests the 5 factors model did not include those two items.

```{r}
FA5_119 <- fa(df2[, -c(9,11)], nfactors = 5, rotate = "varimax", fm = "pa")
print(FA5_119$loadings, cutoff = 0.3, sort = TRUE)
```

The cumulative explained variance of the model decreases. Moreover, factors such as French Lifestyle and Food are merged together, while questions `Im16` and `Im19` have very split loadings between two or three factors. Obviously, this model is not appropriate for our data.

```{r}
rm(FA8_9, FA5_119)
```


Thus, we drop this possible type of model to concentrate on the analysis of variables `Im8`, `Im15` and `Im19`.

## Models including and excluding variables `Im8`, `Im15` and `Im19`.

We remind the reader that these three variables in the `FA8_119` model had split loadings onto 
were loadings on factors that make logical sense, but the values of loadings were quite close to the 0.3 cut-off.

We explore all combinations of models with 8 factors and with or without the three items of interest.

```{r}
FA8_119_81519 <- fa(df2[, -c(9,11,8,15,19)], nfactors = 8, rotate = "varimax", fm = "pa")
FA8_119_815   <- fa(df2[, -c(9,11,8,15)],    nfactors = 8, rotate = "varimax", fm = "pa")
FA8_119_819   <- fa(df2[, -c(9,11,8,19)],    nfactors = 8, rotate = "varimax", fm = "pa")
FA8_119_1519  <- fa(df2[, -c(9,11,15,19)],   nfactors = 8, rotate = "varimax", fm = "pa")
FA8_119_8     <- fa(df2[, -c(9,11,8)],       nfactors = 8, rotate = "varimax", fm = "pa")
FA8_119_15    <- fa(df2[, -c(9,11,15)],      nfactors = 8, rotate = "varimax", fm = "pa")
FA8_119_19    <- fa(df2[, -c(9,11,19)],      nfactors = 8, rotate = "varimax", fm = "pa")
```



### Models excluding one variable

We plot the loadings and print them numerically for `FA8_119_8`, `FA8_119_15` and `FA8_119_19`.

```{r}
par(mfrow = c(3,1))
corrplot(t(FA8_119_8$loadings),
         tl.cex = 0.7,
         title = "Loadings of PAF with 8 factors excluding Images 9, 11 and 8",
         mar = c(0, 1, 3, 0))
corrplot(t(FA8_119_15$loadings),
         tl.cex = 0.7,
         title = "Loadings of PAF with 8 factors excluding Images 9, 11 and 15",
         mar = c(0, 1, 3, 0))
corrplot(t(FA8_119_19$loadings),
         tl.cex = 0.7,
         title = "Loadings of PAF with 8 factors excluding Images 9, 11 and 19",
         mar = c(0, 1, 3, 0))
par(mfrow = c(1,1))
```


```{r}
print(FA8_119_8$loadings, cutoff = 0.3, sort = TRUE)
print(FA8_119_15$loadings, cutoff = 0.3, sort = TRUE)
print(FA8_119_19$loadings, cutoff = 0.3, sort = TRUE)
```

All three models have a total explained variance which has improved when compared to `FA8_119`. The biggest improvement is when removing `Im15`. However, we see that in all three models, the two remaining variables of interest are still loading between various factors, with no remarkable attachment to a particular one. Thus, we probably will get more satisfying results when looking at models excluding two variables.

### Models excluding two variables

We plot the loadings and print them numerically for `FA8_119_815`, `FA8_119_819` and `FA8_119_1519`.

```{r}
par(mfrow = c(3,1))
corrplot(t(FA8_119_815$loadings),
         tl.cex = 0.7,
         title = "Loadings of PAF with 8 factors excluding Images 9, 11, 8 and 15",
         mar = c(0, 1, 3, 0))
corrplot(t(FA8_119_819$loadings),
         tl.cex = 0.7,
         title = "Loadings of PAF with 8 factors excluding Images 9, 11, 8 and 19",
         mar = c(0, 1, 3, 0))
corrplot(t(FA8_119_1519$loadings),
         tl.cex = 0.7,
         title = "Loadings of PAF with 8 factors excluding Images 9, 11, 15 and 19",
         mar = c(0, 1, 3, 0))
par(mfrow = c(1,1))
```


```{r}
print(FA8_119_815$loadings, cutoff = 0.3, sort = TRUE)
print(FA8_119_819$loadings, cutoff = 0.3, sort = TRUE)
print(FA8_119_1519$loadings, cutoff = 0.3, sort = TRUE)
```

Here, model `FA8_119_815` outperforms the other two. Its explained variance is the highest we have seen yet, at 0.800, and the loadings are clear for all variables. This is the first instance of variable `Im19` having a loading which is above the usual 0.6 cutoff. This is a good sign. The other two models still exhibits split loadings for the three problematic variables. 

### Model excluding the three variables


```{r}
corrplot(t(FA8_119_81519$loadings),
         tl.cex = 0.7,
         title = "Loadings of PAF with 8 factors excluding Images 9, 11, 8, 15 and 19",
         mar = c(0, 1, 3, 0))
```

```{r}
print(FA8_119_81519$loadings, cutoff = 0.3, sort = TRUE)
```

When excluding all three variables, we see that `Im16` becomes loaded by itself onto one factor, and its loading is split between three factors. This is due to the fact that `Im19` is essential to form a factor together with `Im16`. The total explained variance is very slightly higher than before, but a difference of 0.001 is not a good trade-off to accepting split loadings in `Im16`.


## Final model of choice

As we have shown, the best model we have found is `FA8_119_815`. It has an explained variance of 80% and all factor loadings are clear and above 0.6. We remove all other models.

```{r}
rm(FA8_119, FA8_119_8, FA8_119_15, FA8_119_19, FA8_119_819, FA8_119_1519, FA8_119_81519)
```

We display the loadings of our final model again for clarity.

```{r}
print(FA8_119_815$loadings, cutoff = 0.3, sort = TRUE)
```


# FIRST QUESTION: What are the dimensions by which Galeries Lafayette is perceived? Please explain your findings and rationale for your final result.

Our final choice if `FA8_119_815` The 8 factors identified are:


**Factor 1:** Organization and Arrangement

```{r}
descriptions[c(3,4,5)]
```

**Factor 2:** Food

```{r}
descriptions[c(10,14)]
```

**Factor 3:** Shopping experience

```{r}
descriptions[c(20,21,22)]
```

**Factor 4:** "Coolness" factor of the Galerie

```{r}
descriptions[c(17,18)]
```

**Factor 5:** Assortment

```{r}
descriptions[c(1,2)]
```

**Factor 6:** French Lifestyle

```{r}
descriptions[c(6,7)]
```

**Factor 7:** Luxury and Designer brands' presence

```{r}
descriptions[c(12,13)]
```

**Factor 8:** Professional appearance

```{r}
descriptions[c(16,19)]
```

We can rename our eight factors inside the table which describes the total variance and the total explained variance.

```{r}
colnames(FA8_119_815$Vaccounted) <- c('Organization', 'Experience', 'Assortment', 'Food',
                                      'France', 'Coolness', 'Luxury', 'Professionalism')
FA8_119_815$Vaccounted
```

This allows us to quickly identify which are the most important factors. To facilitate the visualisation, we plot a Pareto chart for the total explained variance (TVE) below.

```{r}
ggplot(mapping = aes(x = reorder(colnames(FA8_119_815$Vaccounted), -FA8_119_815$Vaccounted[4,]),
                     y = FA8_119_815$Vaccounted[4,])) +
  geom_col() +
  geom_line(aes(x = 1:8, y = FA8_119_815$Vaccounted[5,]), color = "red") +
  geom_point(aes(x = 1:8, y = FA8_119_815$Vaccounted[5,])) +
  theme_classic() +
  labs(x = "Factor",
       y = "Proportion of TVE",
       title = "Pareto chart of TVE for FA8_119_815")
```

Organisation and Experience are the two most important factors in our model, they are the two constructs that people identify the most with Galeries Lafayette. After Experience, there is a small gap, and afterwards factors decrease very slowly in terms of their TVE portion, except for Professionalism at the end which is a bit lower. This means that the way Galeries Lafayette are organized in terms of displays and the atmosphere giving customers a unique experience are the two most important factors for the Galeries.

# Confirmatory Analysis

The "outcomes" for our models will be Repurchase intention and
Cocreation Intention.

The "mediators" will be "customer Satisfaction" and "Affective
Commitment".

### Initial factors

CFA is used to test whether the design construct ( in our case
`FA8_119_815`) is actually appropriate in modelling the factors. In the
exploratory part we first developed a hypothesis of what latent factors
are described by the observable variables (Images).

We state the model:

`=~` means "is measured by", what we are designing here is a measurement
model, because the latent variables predict the observed exogenous
variables. What `lavaan` does by default is correlating the factors, to
avoid that we could `factor1~~0*factor2` and so on for all 8 factors.
But this choice depends on the sample data we have, if we were to find a
bad fit for this model then this is something we might want to consider
looking into.

```{r}
construct <- "
Organization=~Im3+Im4+Im5
Food=~Im10+Im14
Shop_experience=~Im20+Im21+Im22
Coolness=~Im17+Im18
Assortment=~Im1+Im2
French_lifestyle=~Im6+Im7
Luxury=~Im12+Im13
Professinalism=~Im16+Im19 
"

fit <- lavaan::cfa(construct, data = df1, missing = "ML")
summary(fit, fit.measures = TRUE, standardized = TRUE)
```



We analyse the output.

-   Chi-square/df= 1.90 which is good is should be below 3 or below 5
    for a 1000 observations sample (we have 533), this indicates a good
    fit. The p-value for the Chi-square statistic is 0.00, we reject the
    null hypothesis (exact fit$\Sigma(\theta)=\Sigma$) which means that
    our empirical sample covariance matrix S is any positive definite
    matrix. This depends on the sample size, with a larger sample size
    we are more likely to reject the null hypothesis of the Chi-square
    test, which is also why we proceed with observing other measures of
    fit.

    **Incremental**

-   CFI\>0.95 which is good, we accept the model. Our model is better
    than the baseline model with uncorrelated variables.

-   TFI\>0.95 as well, which is good.

    **Absolute**

-   RMSEA is below 0.6.

We see that the first items in the factors estimates (under latent
variables) are fixed to 1, that is because lavaan uses the Marker Method
for identification, by default, which is also why standard error z-stat
and P value are not estimated for the first loading. By fixing that
loading to 1 our model is at least identified (in our case df=107 so it
is over-identified). Moreover, what this fixing to 1 does is that is
scales the loadings (and the variances) to the scale of that item being
fixed, for it is not necessarily crucial because all questions are over
the same 7 item scale. So if we consider the Assortment factor, a 1 unit
increase in Im1 will increase Im2 by 0.88.

We also see that the factors variances are scaled to 1, and that the
residuals of the Images ( because they have the dot . in front of the
name) have a variance, because as stated before they are being predicted
by the latent factors.

The standardised loadings are all above 0.6 ( scaled as well and
therefore comparable, with -1 as minimum values and 1 as maximum value),
meaning that the Images in each factors are good indicators for the
latent variables. Those values if squared represent the item's variance
that is explained through the construct.

We plot our model:

```{r}
lavaanPlot(name="plot_factors", fit, labels = NULL)
```

It is the latent factors that predict the observed items, not the other
way around.

We want to try and understand why the Chi-Square test was significant.
Overall, the model we design through EFA is good.

```{r}
modindices(fit, minimum.value = 10, sort. = TRUE)
```

The `mi` values are not super high which is comforting, we try to add
Im21\~\~Im22 to the model and see if that brings improvement. What the
output tells us is that image 21 and image 22 share variance that is not
explained through the construct.

```{r}
construct2 <- "
Assortment=~Im1+Im2
French_lifestyle=~Im6+Im7
Shop_experience=~Im20+Im21+Im22
Organization=~Im3+Im4+Im5
Luxury=~Im12+Im13
Coolness=~Im17+Im18
Food=~Im10+Im14
Professinalism=~Im16+Im19 
Im21~~Im22
"

fit2<-cfa(construct2, data=df1, missing="ML")
summary(fit2, fit.measures=TRUE, standardized=TRUE)
```

We report only the first covariance introduction (others were
performed), very little have changed in terms of performance metrics. We
keep the model as it is.


### Mediators

We proceed with a stepwise construction of the model, which is
structural since we are introducing a relationship between latent
variables.

The paths between latent variables ( so the factors, the mediator COM
and SAT) are modelled as regression paths.

```{r}
construct_m <- "
# MEASUREMENT MODELS
# Factors
Assortment=~Im1+Im2
French_lifestyle=~Im6+Im7
Shop_experience=~Im20+Im21+Im22
Organization=~Im3+Im4+Im5
Luxury=~Im12+Im13
Coolness=~Im17+Im18
Food=~Im10+Im14
Professionalism=~Im16+Im19

# mediators
COM=~COM_A1+COM_A2+COM_A3+COM_A4
SAT=~SAT_1+SAT_2+SAT_3

# STRUCTURAL MODELS
# Paths from factors to mediators (latent to latent)
COM ~ a*Assortment + b*French_lifestyle + c*Shop_experience + d*Organization + e*Luxury + f*Coolness + g*Food + h*Professionalism

SAT ~ i*Assortment + l*French_lifestyle + m*Shop_experience + n*Organization + o*Luxury + p*Coolness + q*Food + r*Professionalism

# Total effects for mediators
# to COM
A_COM := a
Fr_COM:=b
Sh_COM:=c
Org_COM:=d
Lux_COM:=e
Cool_COM:=f
Food_COM:=g
Prof_COM:=h

# to SAT
A_SAT:=i
Fr_SAT:=l
Sh_SAT:=m
Org_SAT:=n
Lux_SAT:=o
Cool_SAT:=p
Food_SAT:=q
Prof_SAT:=r

"
fitm<-cfa(construct_m, data=df1, missing="ML")
summary(fitm, fit.measures=TRUE, standardized=TRUE)


```

The model remains of good fit: CHi-square/df= 1,52, CFI and TFI above
0.95, RMSEA below 0.6, SRMR\< 0.7.

When we observe the latent variables output, we notice that all
standardised loadings are above 0.6, SAT_3 by little but it is
acceptable. COM and SAT are good measures.

```{r}
semPaths(fitm, what = "path", whatLabels = "std", style = "lisrel",fade=TRUE,
         rotation = 2, layout = "tree3", mar = c(1, 1, 1, 1), 
         nCharNodes = 7,shapeMan = "rectangle", sizeMan = 8, sizeMan2 = 5, 
         curvePivot=TRUE, edge.label.cex = 1, edge.color = "darkblue")
```

The path linking COM and SAT is because we are allowing for the two to
predict each-other.


### Complete Model

Customer Satisfaction coded as SAT Affective Commitment coded as COM

Repurchase Intention coded as C_REP Cocreation Intention C_CR.

The paths between latent variables ( so the factors, the mediator COM
and SAT, and the outcomes C_REP and C_CR ) are modelled as regression
paths.

`allora questo cancellalo, ma ci sono delle espressioni che ho scritto che sono commented out perchè redundant per R. ho fatto riferimento alla slide 28 dell folder 04 delle slide.`

```{r}
construct3 <-"
# MEASUREMENT MODELS
# Factors
Assortment=~Im1+Im2
French_lifestyle=~Im6+Im7
Shop_experience=~Im20+Im21+Im22
Organization=~Im3+Im4+Im5
Luxury=~Im12+Im13
Coolness=~Im17+Im18
Food=~Im10+Im14
Professionalism=~Im16+Im19

# mediators
COM=~COM_A1+COM_A2+COM_A3+COM_A4
SAT=~SAT_1+SAT_2+SAT_3

# Outcomes
C_REP=~ C_REP1+C_REP2+C_REP3
C_CR=~C_CR1+C_CR2+C_CR3+C_CR4


# STRUCTURAL MODELS
# Paths from factors to mediators
COM ~ a*Assortment + b*French_lifestyle + c*Shop_experience + d*Organization + e*Luxury + f*Coolness + g*Food + h*Professionalism

SAT ~ i*Assortment + l*French_lifestyle + m*Shop_experience + n*Organization + o*Luxury + p*Coolness + q*Food + r*Professionalism

# Total effects for mediators
# to COM
A_COM:=a
Fr_COM:=b
Sh_COM:=c
Org_COM:=d
Lux_COM:=e
Cool_COM:=f
Food_COM:=g
Prof_COM:=h

# to SAT
A_SAT:=i
Fr_SAT:=l
Sh_SAT:=m
Org_SAT:=n
Lux_SAT:=o
Cool_SAT:=p
Food_SAT:=q
Prof_SAT:=r

# Paths from mediators to outcomes
#C_REP ~ u*COM+v*SAT 
#C_CR ~ w*COM+z*SAT

# Paths from factors to outcome
C_REP  ~ aa*Assortment + bb*French_lifestyle + cc*Shop_experience + dd*Organization + ee*Luxury + ff*Coolness + gg*Food + hh*Professionalism +u*COM +v*SAT

C_CR ~ ii*Assortment + ll*French_lifestyle + mm*Shop_experience + nn*Organization + oo*Luxury + pp*Coolness + qq*Food + rr*Professionalism + w*COM+ z*SAT

# Indirect paths from factors to outcomes
# passing through COM (edges [a,...,h] and [u,w])
#C_REP ~ ua*Assortment + ub*French_lifestyle + uc*Shop_experience + ud*Organization + ue*Luxury + uf*Coolness + ug*Food + uh*Professionalism

#C_CR ~ wa*Assortment + wb*French_lifestyle + wc*Shop_experience + wd*Organization + we*Luxury + wf*Coolness + wg*Food + wh*Professionalism

# define indirect effects through COM
# to C_REP
ua:=a*u
ub:=b*u
uc:=c*u
ud:=d*u
ue:=e*u
uf:=f*u
ug:=g*u
uh:=h*u

# to C_CR
wa:=a*w
wb:=b*w
wc:=c*w
wd:=d*w
we:=e*w
wf:=f*w
wg:=g*w
wh:=h*w

# passing through SAT (edges [i,...,r] and [v,z])
#C_REP ~ vi*Assortment + vl*French_lifestyle + vm*Shop_experience + vn*Organization + vo*Luxury + vp*Coolness + vq*Food + vr*Professionalism

#C_CR ~ zi*Assortment + zl*French_lifestyle + zm*Shop_experience + zn*Organization + zo*Luxury + zp*Coolness + zq*Food + zr*Professionalism

# define indirect effects 
# through COM
vi:=i*v
vl:=l*v
vm:=m*v
vn:=n*v
vo:=o*v
vp:=p*v
vq:=q*v
vr:=r*v

# Through SAT
zi:=i*z
zl:=l*z
zm:=m*z
zn:=n*z
zo:=o*z
zp:=p*z
zq:=q*z
zr:=r*z

# Total effects, from factors to outcomes
# to C_REP
A_REP := aa+ua+vi
Fr_REP:=bb+ub+vl
Sh_REP:=cc+uc+vm
Org_REP:=dd+ud+vn
Lux_REP:=ee+ue+vo
Cool_REP:=ff+uf+vp
Food_REP:=gg+ug+vq
Prof_REP:=hh+uh+vr

# to C_CR
A_CR:=ii+wa+zi
Fr_CR:=ll+wb+zl
Sh_CR:=mm+wc+zm
Org_CR:=nn+wd+zn
Lux_CR:=oo+we+zo
Cool_CR:=pp+wf+zp
Food_CR:=qq+wg+zq
Prof_CR:=rr+wh+zr
"

fit3<-cfa(construct3, data=df1, missing="ML")
summary(fit3, fit.measures=TRUE, standardized=TRUE)


```

**Measures of fit**

-   Chi-Square/D=1.64

-   CFI and TFI above 0.95

-   RMSEA measures the degree of misspecification of the model, it
    measures 0.036, which is below the 0.05 indicating that our model is
    a close fit to the true model.

-   acceptability threshold

-   SRMR 0.043

All latent variables have std.all above 0.6.


```{r}
lavaanPlot(name="plot", fit3, labels = NULL)
```

`allora poi cancella anche quest, ma come vedi questo grafico con semapth non riesce a mettere i emdiatori al posto giusto, ora if you change the layout with tree,tre2,tree3 vedi che cambia ma non riesco a metterli in mezzo. pero lavaanplot va benissimo e quindi volevo si puo anche togliere`

```{r}
semPaths(fit3,what = "path", whatLabels = "std", style = "lisrel",exoCov = T,
         rotation = 2, layout = "tree", mar = c(1, 2, 1, 2), 
         nCharNodes = 7,shapeMan = "rectangle", sizeMan = 7, sizeMan2 = 5, 
         curvePivot=TRUE, edge.label.cex = 1.2, edge.color = "darkblue")


semPaths(fit3, what="std", fade=F, edge.label.cex=1, sizeMan=7, sizeLat=7, exoCov = F, exoVar = F, intercepts = F, rotation = 2)

semPaths(fit3, what="path", whatLabels="std")
```

```{r}
construct_trial <- "
# MEASUREMENT MODELS
# Factors
Assortment=~Im1+Im2
French_lifestyle=~Im6+Im7
Shop_experience=~Im20+Im21+Im22
Organization=~Im3+Im4+Im5
Luxury=~Im12+Im13
Coolness=~Im17+Im18
Food=~Im10+Im14
Professionalism=~Im16+Im19

# mediators
COM=~COM_A1+COM_A2+COM_A3+COM_A4
SAT=~SAT_1+SAT_2+SAT_3

# Outcomes
C_REP=~ C_REP1+C_REP2+C_REP3
C_CR=~ C_CR1+C_CR2+C_CR3+C_CR4

# STRUCTURAL MODELS
# latent factors(exogenous) to latent mediator (endogenuous)
COM ~ Assortment + French_lifestyle + Shop_experience + Organization + Luxury + Coolness + Food + Professionalism

SAT ~ Assortment + French_lifestyle + Shop_experience + Organization + Luxury + Coolness + Food + Professionalism

#latent factors (exogenous) to latent outcome factors (endogenous)
C_REP  ~ Assortment + French_lifestyle + Shop_experience + Organization + Luxury + Coolness + Food + Professionalism +COM +SAT

C_CR ~ Assortment + French_lifestyle + Shop_experience + Organization + Luxury + Coolness + Food + Professionalism + COM + SAT"

fit_trial<- sem(construct_trial, data=df1, missing="ML")
summary(fit_trial,fit.measures=TRUE, standardized=TRUE)


```



#### Reliability of Exogenous and Endogenous Constructs

Construct reliability can be measured either with Cronbach's alpha or with Composite Reliability (CR). The former is less reliable of a measure. Regardless, if either one are above 0.95 it means that the observables forming the factor are measuring the same thing, making them redundant.

**Cronbach's alpha**

```{r}

cronbach(subset(df1, select = c(Im1,Im2)))$alpha
cronbach(subset(df1, select = c(Im6,Im7)))$alpha
cronbach(subset(df1, select = c(Im20,Im21,Im22)))$alpha
cronbach(subset(df1, select = c(Im3,Im4,Im5)))$alpha
cronbach(subset(df1, select = c(Im12,Im13)))$alpha
cronbach(subset(df1, select = c(Im17,Im18)))$alpha
cronbach(subset(df1, select = c(Im10,Im14)))$alpha
cronbach(subset(df1, select = c(Im16,Im19)))$alpha
cronbach(subset(df1, select = c(COM_A1:COM_A4)))$alpha
cronbach(subset(df1, select = c(SAT_1:SAT_3)))$alpha
cronbach(subset(df1, select = c(C_CR1:C_CR4)))$alpha
cronbach(subset(df1, select = c(C_REP1:C_REP3)))$alpha
```

The alphas of the factors are high, all over the 0.70 benchmark. The
items are sufficiently reliable. None is above 0.95, the questions describe the latent factor well without being redundant.


**Composite Reliability**

```{r}
# computing needed parameters
std.loadings<- inspect(fit3, what="std")$lambda
check=std.loadings
check[check>0] <- 1
std.loadings[std.loadings==0] <- NA
std.loadings2 <- std.loadings^2
std.theta<- inspect(fit3, what="std")$theta

#CR
sum.std.loadings<-colSums(std.loadings, na.rm=TRUE)^2
sum.std.theta<-rowSums(std.theta)
sum.std.theta=check*sum.std.theta
CR=sum.std.loadings/(sum.std.loadings+colSums(sum.std.theta))
CR
```

The CR values are high enough, without signaling redundancy.

**Indicator Reliability**
```{r}
#Individual item Reliability
IIR=std.loadings2/(colSums(std.theta)+std.loadings2)
IIR

```

We observe that the reliability for C_CR2 is low. That should be take out of the model.

#### Average Variance Extracted

It is the measure of the amount of variance that is captured by a construct in relation to the amount of variance due to measurement error.

```{r}
std.loadings<- inspect(fit3, what="std")$lambda
std.loadings <- std.loadings^2
AVE=colSums(std.loadings)/(colSums(sum.std.theta)+colSums(std.loadings))
AVE
```


```{r}
std_fit1=inspect(fit3, "std")
std_fit1$psi^2
```


#### Discriminant validity

 Discriminant validity measures the distinctiveness of a
construct. Discriminant validity is demonstrated when the shared variance within a construct (AVE) exceeds the shared variance between the
constructs. 


#### 2.Are the mechanism driving satisfaction and affective commitment similar? Are satisfaction and affective commitment mediating the impact of image perceptions on outcomes? If yes for which outcomes?

#### 3. What is driving the two distinct outcomes (repurchase and co-creation intention): Please rank the image dimensions with respect to the total effect on each outcome? Interpret your results

References:

[Confirmatory Factor Analysis (CFA) in R with lavaan
(ucla.edu)](https://stats.oarc.ucla.edu/r/seminars/rcfa/#s3b)

https://www.sciencedirect.com/science/article/pii/S1071581910001278

https://en.wikipedia.org/wiki/Congeneric_reliability

https://en.wikipedia.org/wiki/Average_variance_extracted

