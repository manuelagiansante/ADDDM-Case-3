---
title: "Case Study 3"
author: "Manuela Giansante & Lucia Camenisch"
date: "2023-04-15"
output:
  html_document:
    toc: true # creating a table of contents (toc)
    toc_float:
      collapsed: false # toc does not collapse and is shown as a sidebar (toc_float)
    number_sections: true # document sections are numbered
    theme: cosmo
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.align = "center")
```

```{r}
library(data.table)      # data.table type
library(rcompanion)      # plothistogram()
library(tidyr)           # gather()
library(kableExtra)      # kables for html document
library(naniar)          # vis_miss()
library(corrplot)        # corrplot()
library(caret)           # findCorrelation()
library(REdaS)           # KMOS(), bart_spher()
library(psych)           # KMO(), fa(), principal()
library(lavaan) 
library(semPlot)         # for visualization
library(psy)
library(lavaanPlot)
```

## Galeries Lafayette

We load the data files.

```{r}
descriptions <- fread("Variables and Labels_Galeries Lafayette.csv")
df <- read.csv("Case Study III_Structural Equation Modeling.csv")

```

We investigate missing values in the data, we know that missing values
have been coded as "999" in the dataset:

```{r}
mean(df==999)# percentage of missing data
df1<- data.frame(sapply(df,function(x) ifelse((x==999),NA,as.numeric(x))))

# Replace NA with column mean
for (i in 1:ncol(df1)) {
  col_mean <- mean(df1[,i], na.rm = TRUE)
  df1[is.na(df1[,i]), i] <- col_mean
}

# separate dataframe only with the questionaire data
df2<- df1[,c(1:22)]

```

We have almost 3% of missing data, deleting it would mean we would work
with a little more than half the data we have now. We therefore replace
the 999 values with the column mean.

## Exploratory Factor Analysis

We first plot the variables so to have a sense of how client perception
is skewed.

```{r}
# Plot the variables
par(mfrow = c(3, 3))
for (col in c(1:ncol(df2))){ 
  plotNormalHistogram(df2[col],main = paste("Frequency Distribution of Im", col))
  }
```

For most questions the distribution is skewed to the left, answers
leaned towards the "does apply completely" direction. We observe that
for question number 7 asking whether Galeries Lafayette Berlin embody
French Savoir-vivre for the client, none has answered "not at all",
because the range of the distribution starts from 2. Question 10 about
whether the Galeries represent gourmet food has also received no
completely negative answers answering not at all signals that brand
identity

We run a correlation matrix so to see if we can individuate some
groupings in our variables.

```{r}
raqMatrix <- cor(df2)
corrplot(raqMatrix, order = 'hclust', tl.cex = 0.8, addrect = 10)
```

The clusters that form just by observing correlation between variable
are a first suggestions of what kind of factors we can identify. We
check the description files to see if the clusters are thematically
coherent:

-   Im8, Im10, Im14: They all regard food and the quality of food
    (French cuisine, gourmet food);

-   Im6, Im7: French Lifestyle;

-   Im12, Im13: Luxury and designer brands;

-   Im20,Im21, Im22: They are about the feeling of the shopping
    experience, so whether the customer feels at ease;

-   Im16, Im19: They are about the perception of professionalism;

-   Im15, Im1, Im2: They cover the brand/ product assortment;

-   Im18, Im17: Describe how on trend the Galeries are perceived to be;

-   Im5, Im3, Im4: Measure how the client perceives the decorations and
    arrangement of the shopping areas.

Variables Im9, Im11 seem to stand on their own in terms of correlations,
in this order the respective questions concern French Fashion and French
Cosmetics. Especially the latter one seems to be isolated in terms of
correlation and meaning.

```{r}
# Kaiser factor adequacy
KMO(df2)
```

We computed the Overall measure sampling adequacy is 0.88 the variables
seem adequate enough for factor analysis. The lowest MSA is achieved by
Im2, but still it is a quite high value so we should not worry too
much.Moreover, the correlation grouping it formed with Im15 and Im1 had
a strong common theme. Im9 and Im11 both have high MSA which given that
they stood on their own in the correlation matrix it is reassuring.

```{r}
anti_mat_diag = data.frame("Question" = 1:22,
                           "UniqueVar" = diag(KMO(df2)$ImCov))

ggplot(data = anti_mat_diag, aes(x = Question, y = UniqueVar)) +
  geom_col() +
  theme_classic() +
  scale_x_continuous(breaks = seq(1, 22, 1)) +
  scale_y_continuous(limits = c(0,1)) +
  labs(title = "Diagonal values of anti-image correlation matrix",
       y = "Proportion of unique variance")
```

We plot a bar chart of the unique variances, there not seem to be very
big outliers, yet Im9 and Im11 do stand out, we saw how those questions
were unique in terms of the matter addressed, so definitely the
questionnaire could be improved by going deeper into the french fashion
and french cosmetics subjects. This is also considering that marketing
wise these 2 are very high selling points especially for tourists who
might come and look for something more "peculiar" and identifiable as
being from Paris.

```{r}
# Barlett's Test of Sphericity
bart_spher(df2)
```

We are testing whether the sample of data we have stems from a
population of uncorrelated variables, meaning the correlation matrix is
an identity matrix (diagonal of 1s).The test is significant, the
correlation matrix is not an identity matrix, there is correlation among
the variables.

#### Principal Axis Factoring

We use a starting number of factors 15 to avoid errors from the system
but still get a a fair number of runs. For clarity we describe the used
criteria, so the have a more readable plot:

-   fit, which measures how well the factor model reproduces the
    correlation matrix;

-   objective, which is the value of the function that is minimized by a
    maximum likelihood procedure;

-   rms, which is the sum of the squared off-diagonal residuals divided
    by the degrees of freedom;

-   crms, which is the RMS adjusted for degrees of freedom;

-   TLI, which is the Tucker Lewis Index of factoring reliability;

-   BIC, which is the Bayesian information criterion.

```{r}
# we initiate and empty dataframe which will record our criteria values
fit_df <- matrix(nrow = 15, ncol = 6)
colnames(fit_df) <- c("fit", "objective", "rms", "crms", "TLI", "BIC")
fit_df <- as.data.frame(fit_df)

# we compute the factor analysis for nfactors 
for (i in 1:15) {
  FA <- fa(df2, nfactors = i, rotate = "varimax", fm = "pa", SMC = FALSE)
  fit_df[i, 1] <- FA$fit
  fit_df[i, 2] <- FA$objective
  fit_df[i, 3] <- FA$rms
  fit_df[i, 4] <- FA$crms
  fit_df[i, 5] <- FA$TLI
  fit_df[i, 6] <- FA$BIC
}

```

```{r}
fit_df %>% gather() %>%                 
  ggplot(aes(x = rep(1:15, ncol(fit_df)), y = value)) +
  facet_wrap(~ key, scales = "free") +
  geom_point() +
  theme_classic() +
  scale_x_continuous(breaks = seq(1, 15, 1)) +
  geom_vline(xintercept = 8, linetype = "dashed") +
  geom_vline(xintercept = 9, linetype = "dashed") +
  labs(title = "Various criteria for Principal Axis Factoring",
       x = "Number of factors",
       y = "")
```

It seems that the number of factors we want to try and fit is 8 or 9.

### Full models with 9 or 8 factors

We start fitting our first models, of course selecting our final model
will be a process of trial and error.

```{r, message=FALSE}
# including all images
FA9 <- fa(df2, nfactors = 9, rotate = "varimax", fm = "pa")
FA8 <- fa(df2, nfactors = 8, rotate = "varimax", fm = "pa")

# without images 9 an 11
FA9_119 <- fa(df2[, -c(9,11)], nfactors = 9, rotate = "varimax", fm = "pa")
FA8_119 <- fa(df2[, -c(9,11)], nfactors = 8, rotate = "varimax", fm = "pa")
```

We print the loadings of these models in heatmaps:

```{r}
par(mfrow = c(2,2))
corrplot(t(FA8$loadings),
         tl.cex = 0.7,
         title = "PAF with 8 factors \n Loadings",
         mar = c(0, 1, 3, 0))
corrplot(t(FA8_119$loadings),
         tl.cex = 0.7,
         title = "PAF with 8 factors excluding im9 and im11 \n Loadings",
         mar = c(0, 1, 3, 0))
corrplot(t(FA9$loadings),
         tl.cex = 0.7,
         title = "PAF with 9 factors \n Loadings",
         mar = c(0, 1, 3, 0))
corrplot(t(FA9_119$loadings),
         tl.cex = 0.7,
         title = "PAF with 9 factors excluding im9 and im11 \n Loadings",
         mar = c(0, 1, 3, 0))
```

We plot the loading of all the models we estimated, we notice that the
comprehensive models perform badly on the loadings for Im9 on factor 2,
Im11 is not as low on factor 5 but it is not brilliant.

### Models without Image 9 (9 and 8 factors)

We investigate what the loadings look like in case we only take out the
Im9:

```{r}
FA9_9 <- fa(df2[, -c(9)], nfactors = 9,rotate="varimax", fm = "pa")
FA8_9 <- fa(df2[, -c(9)], nfactors = 8, rotate = "varimax", fm = "pa")

par(mfrow = c(1,2))
corrplot(t(FA8_9$loadings),
         tl.cex = 0.7,
         title = "PAF with 9 factors excluding im9 \n Loadings",
         mar = c(0, 1, 3, 0))
corrplot(t(FA9_9$loadings),
         tl.cex = 0.7,
         title = "PAF with 9 factors excluding im9 \n Loadings",
         mar = c(0, 1, 3, 0))
```

The performance of Im11 in terms of loadings on factors does not
improve, which conceptually makes sense because the 2 variables describe
something quite different. We get rid of the full models and print the
loadings for remaining ones:

```{r}
print(FA8_9$loadings, cutoff = 0.3, sort = TRUE)
print(FA9_9$loadings, cutoff = 0.3, sort = TRUE)
print(FA9_119$loadings, cutoff = 0.3, sort = TRUE)
print(FA8_119$loadings, cutoff = 0.3, sort = TRUE)
```

Im8 loading on PA2 is quite low, we keep that in mind for further
analysis.

The model with the highest cumulative variance is the FA9_911, but if we
observe more carefully we notice that no variables are loading on factor
PA9. Therefore, the next best option is model FA8_911. The total
variance explained is 0.763. In this model Im15 and Im19 (very close to
cut off, respectively 0.464 and 0.312) are partially loading on factors
PA4 and PA1, we check again their descriptions to understand why that
might be:

```{r}
descriptions[c(15,19),2]
```

In the correlation matrix they grouped, respectively, with brand
assortment and perception of professionalism. PA4 in our case describes
the client perception of arrangement of the Galeries, variable 9 is
about the organization which logically can be connected. PA1 and
variable 15 are grouped together in the correlation matrix already which
is encouraging, they are about brand assortment. We will investigate
this later on. We will keep these two variables under watch, but for now
these two variables make sense to be loading on the factors they are
loading on. Moreover, the correlation of Im15 with the other variales in
factor 4 struggles to go above 0.5. So at this stage of the exploratory
analysis we know that the best option if a 8 factor model without images
9 and 11.

### Why 9 factors are not the best option

We further confirm that the 9 factor model would be superfluous by
plotting the Scree graph and taking a closer look at the model Eigen
values:

```{r}
# Scree Plot 9_119
ggplot(mapping = aes(x = 1:length(FA9_119$values),
                     y = FA9_119$values,
                     color = (FA9_119$values >= 1))) +
  geom_point() +
  geom_hline(yintercept = 1, linetype = "dashed") +
  theme_classic() +
  labs(title = "Scree plot of FA9_119",
       x = "Factor Number",
       y = "Eigenvalue",
       color = "Eigenvalue >= 1")
```

Above we have plotted out a Scree plot, which serves the purpose of
determining the number of factors to retain in the Factor Axis Analysis.
Factors Eigen values are ordered in a descending order, the method is
quite subjective but it might provide provide further insight.The scree
plot for the model FA9_119 suggests, by the elbow rule to cut off at
factor number 8, which is coherent, we saw how no variable would load on
the ninth factor. Meaning that PA9 explains no variation in the
variables, it brings no further aid in explaining the structure of our
data.

```{r}
# Print Variances and Eigenvalues
FA9_119$Vaccounted
t(FA9_119$values)
t(cumsum(FA9_119$e.values/ncol(df2)))
```

We notice that the proportion explained by factor 9 is extremely low.
This reinforces how the best option seems to be only retain 8 factors.
We have also printed out Eigenvalues, which explain how much variance in
the original variables is explained by the factors. We follow the
Kaiser-Guttman criterion, the suggested amount of factors to maintain is
6 when observing the eigenvalues. Now this open new space for
experimenting with new models with a different number of factors, yet we
want to point out that the groupings that naturally formed

We briefly recap before moving on with the investigation:

-   Model FA8_119 reaches a cumulative variance of 0.763;
-   we need to pay attention to the variables Im15 and Im19;
-   Eigen Values from FA9_119 actually suggest a 6 factors model

### Further investigation on FA8_119

```{r}
# Scree Plot FA8_119
ggplot(mapping = aes(x = 1:length(FA8_119$values),
                     y = FA8_119$values,
                     color = (FA8_119$values >= 1))) +
  geom_point() +
  geom_hline(yintercept = 1, linetype = "dashed") +
  theme_classic() +
  labs(title = "Scree plot of FA8_119",
       x = "Factor Number",
       y = "Eigenvalue",
       color = "Eigenvalue >= 1")
```

```{r}
# Print Variances and Eigenvalues
FA8_119$Vaccounted
t(FA8_119$values)
t(cumsum(FA8_119$e.values/ncol(df2)))
```

By considering less factors the "elbow rule" turns even more
conservative, suggesting a four factor model. We test some models with
less than 8 factors to see if we can outdo the 0.763 explained variance
of FA8_119.

### Six factors models

The first model we run is one with only six factors and without Im9 and
Im11, because we reached the 6 factor conclusion in models not
considering those Images.

```{r}
FA6_119 <- fa(df2[, -c(9,11)], nfactors = 6,rotate="varimax", fm = "pa")
print(FA6_119$loadings, cutoff = 0.3, sort = TRUE)

```

The cumulative explained variance does not increase. Im15 and Im19
increase in terms of loadings, but of course the factors are not the
same anymore. For example Im6, Im7, Im8, Im10, Im14 are all loading on
factor2, with high loadings as well.

```{r}
descriptions[c(6,7,8,10,14)]
```

When we recall what aspects these questions cover we see that a "French
lifestyle" theme has been clustered together with Gourmet Food, which
are aspects that not necessarily go together. These images cover very
different subjects, the 6 factor models pulls together variables that
should not go together. Referring back to the original correlation plot,
Im8 and Im7 correlation is quite high, but for the other variables the
correlation between them is quite underwhelming. This high interaction
between Im8 and Im7 is probably what drives the forming of this factor
2.

So for now we set aside 6 factor models, but will proceed in
investigating more regarding Im15 and Im19.

### Variables Im15 and Im19, 8 factors

#### Exclude both Im15 and Im19

So we remind the reader that these two variables in the FA8_119 model
were loadings on factors that make logical sense, but the loadings
values were quite close to the 0.3 cut-off.

```{r}
FA8_1191519 <- fa(df2[, -c(9,11,15,19)], nfactors = 8,rotate="varimax", fm = "pa")
print(FA8_1191519$loadings, cutoff = 0.3, sort = TRUE)
```

The cumulative explained variance has increased to 0.771 by running a 8
factor model excluding Im9, Im11, Im15, Im19. What happens here is that
now Im16 (Professional appearance towards customer) now has a very close
to cut off loading on factor 1 (with Im3, Im4, Im5) which models
decoration and arrangement. All of these have to do with appearence from
a client point of view.

What we observe now that we are back considering 8 factors models, is
that Im8 still has a quite low loading.

#### Excluding Im19

We try running a model without the Im19 because it was the variable with
the lowest loading value in FA8_119.

```{r}
FA8_11919 <- fa(df2[, -c(9,11,19)], nfactors = 8,rotate="varimax", fm = "pa")
print(FA8_11919$loadings, cutoff = 0.3, sort = TRUE)

```

The cumulative variance falls in between FA8_911 and FA8_9111519.This
model for now we set aside. Im15 is split between three factor with very
low loadings, the question regard professional selection of brand.
Thematically maybe it explores an aspect of client perception that
should be investigated further.

#### Excluding Im15

```{r}
FA8_11915 <- fa(df2[, -c(9,11,15)], nfactors = 8,rotate="varimax", fm = "pa")
print(FA8_11915$loadings, cutoff = 0.3, sort = TRUE)

```

We exclude Im15 the explained variance by the model is in parity with
the "slimmer" model FA8_9111519. Im19 loads on two different factors 4
and 8. Factor 8 would form at the start as well when simply observing
correlations, it deals with professionalism. Both loadings on both
factors are below 0.7 which is not necessarily good.

Now at this stage our choice is between a model that sacrifices more
information and one that includes one more variable, but the over all
explained variance is almost the same.

```{r}
FA8_1191519$Vaccounted
FA8_11915$Vaccounted
```

In FA8_11915 factor 8 has a higher Proportion Var.

Communalities are the shared variability with the factors.

```{r}
FA8_1191519$communality
FA8_11915$communality
```

In the case of Im19 0.5841990 of its variability is explained by the
factor it is loading on (PA4 and PA8) in FA8_11915. In the FA8_1191519
communalities the lowest communality is the one of Im5, its loading on
factor 1 in the model 0.64 both values are below acceptability
thresholds, but removing Im5 from factor one would not make much sense
in terms of themes covered.

In FA8_11915 PA4 -\> decoration and arrangement PA8 -\> professional
appearance

```{r}
descriptions[c(16,19),]
```

Thematically it would makes sense to retain Im19 as a variable, given
that PA8 in FA8_11915 has a higher SS loading, and the actual cumulative
variance of the model is slightly higher than FA8_1191519. The loadings
of Im19 are low, we would have Im16 with low loadings as well, and the
two thematically should go together.

### Considering Im8

One last thing we have not considered up until now variable Im8, which
shows low loadings both in FA8_1191519 and in FA8_11915. There are a few
reasons why we are considering this now, first for clarity of the
reader. We have first decided between 8 and 9 factors, we tried the 6
factors route and finally we digged deeper on how to best select between
Im15 and Im19.

Moreover, when we decided to retain Im19 was because otherwise the Im16
loadings would be too low. With FA8_11915 we reach an acceptable
explained variance, yet Im8 continues to have low loadings on PA2 and
PA7.

```{r}
descriptions[8,2]
```

The matter covered by Im8 seems to both touch the food theme and the
French lifestyle thematic. It seems like this question blurs factors
domains, we run a model without to see if it is worth it keeping it in
our model.

```{r}
FA8_119158 <- fa(df2[, -c(8,9,11,15)], nfactors = 8,rotate="varimax", fm = "pa")
print(FA8_119158$loadings, cutoff = 0.3, sort = TRUE)
```

The resulting Cumulative explained variance is higher than FA8_1191519
and FA8_11915, all loadings are above 0.6. We conclude, our final choice
of model is FA8_119158.

### Final model of choice

#### 1. What are the dimensions by which Galeries Lafayette is perceived?

Our final choice if FA8_11915. The 8 factors identified are:

**Factor 1:** Assortment

```{r}
descriptions[c(1,2)]
```

**Factor 2:** French Lifestyle

```{r}
descriptions[c(6,7)]
```

**Factor 3:** Shopping experience

```{r}
descriptions[c(20,21,22)]
```

**Factor 4:** Organization and Arrangement

```{r}
descriptions[c(3,4,5)]
```

**Factor 5:** Luxury and Designer brands' presence

```{r}
descriptions[c(12,13)]
```

**Factor 6:** "Coolness" factor of the Galerie

```{r}
descriptions[c(17,18)]
```

**Factor 7:** Food

```{r}
descriptions[c(10,14)]
```

**Factor 8:** Professional appearance

```{r}
descriptions[c(16,19)]
```

## Confirmatory Analysis

The "outcomes" for our models will be Repurchase intention and
Cocreation Intention.

The "mediators" will be "customer Satisfaction" and "Affective
Commitment".

### Initial factors

CFA is used to test whether the design construct ( in our case
FA8_119158) is actually appropriate in modelling the factors. In the
exploratory part we first developed a hypothesis of what latent factors
are described by the observable variables (Images).

We state the model:

`=~` means "is measured by", what we are designing here is a measurement
model, because the latent variables predict the observed exogenous
variables. What lavaan does by default is correlated the factors, to
avoid that we could `factor1~~0*factor2` and so on for all 8 factors.
But this choice depends on the sample data we have, if we were to find a
bad fit for this model then this is something we might want to consider
looking into.

```{r}
# repopulated complete data-set
head(df1)

construct <- "
Assortment=~Im1+Im2
French_lifestyle=~Im6+Im7
Shop_experience=~Im20+Im21+Im22
Organization=~Im3+Im4+Im5
Luxury=~Im12+Im13
Coolness=~Im17+Im18
Food=~Im10+Im14
Professinalism=~Im16+Im19 
"

fit<-cfa(construct, data=df1, missing="ML")
summary(fit, fit.measures=TRUE, standardized=TRUE)


```

We analyse the output.

-   Chi-square/df= 1.85 which is good is should be below 3 or below 5
    for a 1000 observations sample (we have 533), this indicates a good
    fit. The p-value for the Chi-square statistic is 0.00, we reject the
    null hypothesis (exact fit$\Sigma(\theta)=\Sigma$) which means that
    our empirical sample covariance matrix S is any positive definite
    matrix. This depends on the sample size, with a larger sample size
    we are more likely to reject the null hypothesis of the Chi-square
    test, which is also why we proceed with observing other measures of
    fit.

    **Incremental**

-   CFI\>0.95 which is good, we accept the model. Our model is better
    than the baseline model with uncorrelated variables.

-   TFI\>0.95 as well, which is good.

    **Absolute**

-   RMSEA is below 0.6.

We see that the first items in the factors estimates (under latent
variables) are fixed to 1, that is because lavaan uses the Marker Method
for identification, by default, which is also why standard error z-stat
and P value are not estimated for the first loading. By fixing that
loading to 1 our model is at least identified (in our case df=107 so it
is over-identified). Moreover, what this fixing to 1 does is that is
scales the loadings (and the variances) to the scale of that item being
fixed, for it is not necessarily crucial because all questions are over
the same 7 item scale. So if we consider the Assortment factor, a 1 unit
increase in Im1 will increase Im2 by 0.80.

We also see that the factors variances are scaled to 1, and that the
residuals of the Images ( because they have the dot . in front of the
name) have a variance, because as stated before they are being predicted
by the latent factors.

The standardised loadings are all above 0.6 ( scaled as well and
therefore comparable, with -1 as minimum values and 1 as maximum value),
meaning that the Images in each factors are good indicators for the
latent variables. Those values if squared represent the item's variance
that is explained through the construct.

We plot our model:

```{r}
lavaanPlot(name="plot_factors", fit, labels = NULL)
```

It is the latent factors that predict the observed items, not the other
way around.

We want to try and understand why the Chi-Square test was significant.
Overall, the model we design through EFA is good.

```{r}
modindices(construct,minimum.value = 10, sort=TRUE)
```

The `mi` values are not super high which is comforting, we try to add
Im21\~\~Im22 to the model and see if that brings improvement. What the
output tells us is that image 21 and image 22 share variance that is not
explained through the construct.

```{r}
construct2 <- "
Assortment=~Im1+Im2
French_lifestyle=~Im6+Im7
Shop_experience=~Im20+Im21+Im22
Organization=~Im3+Im4+Im5
Luxury=~Im12+Im13
Coolness=~Im17+Im18
Food=~Im10+Im14
Professinalism=~Im16+Im19 
Im21~~Im22
"

fit2<-cfa(construct2, data=df1, missing="ML")
summary(fit2, fit.measures=TRUE, standardized=TRUE)
```

We report only the first covariance introduction (others were
performed), very little have changed in terms of performance metrics. We
keep the model as it is.

```{r}
x<-summary(fit2, fit.measures=TRUE, standardized=TRUE)
#Explained variance
(x$pe$std.all)^2
```

#### Reliability

```{r}
#Factors reliability
cronbach(subset(df1, select = c(Im1,Im2)))$alpha
cronbach(subset(df1, select = c(Im6,Im7)))$alpha
cronbach(subset(df1, select = c(Im20,Im21,Im22)))$alpha
cronbach(subset(df1, select = c(Im3,Im4,Im5)))$alpha
cronbach(subset(df1, select = c(Im12,Im13)))$alpha
cronbach(subset(df1, select = c(Im17,Im18)))$alpha
cronbach(subset(df1, select = c(Im10,Im14)))$alpha
cronbach(subset(df1, select = c(Im16,Im19)))$alpha

```

The alphas of the factors are high, all over the 0.70 benchmark. The
items are sufficiently reliable.

### Introduction of mediators

We proceed with a stepwise construction of the model, which is
structural since we are introducing a relationship between latent
variables.

The paths between latent variables ( so the factors, the mediator COM
and SAT) are modelled as regression paths.

```{r}
construct_m <- "
# MEASUREMENT MODELS
# Factors
Assortment=~Im1+Im2
French_lifestyle=~Im6+Im7
Shop_experience=~Im20+Im21+Im22
Organization=~Im3+Im4+Im5
Luxury=~Im12+Im13
Coolness=~Im17+Im18
Food=~Im10+Im14
Professionalism=~Im16+Im19

# mediators
COM=~COM_A1+COM_A2+COM_A3+COM_A4
SAT=~SAT_1+SAT_2+SAT_3

# STRUCTURAL MODELS
# Paths from factors to mediators (latent to latent)
COM ~ a*Assortment + b*French_lifestyle + c*Shop_experience + d*Organization + e*Luxury + f*Coolness + g*Food + h*Professionalism

SAT ~ i*Assortment + l*French_lifestyle + m*Shop_experience + n*Organization + o*Luxury + p*Coolness + q*Food + r*Professionalism

# Total effects for mediators
# to COM
A_COM := a
Fr_COM:=b
Sh_COM:=c
Org_COM:=d
Lux_COM:=e
Cool_COM:=f
Food_COM:=g
Prof_COM:=h

# to SAT
A_SAT:=i
Fr_SAT:=l
Sh_SAT:=m
Org_SAT:=n
Lux_SAT:=o
Cool_SAT:=p
Food_SAT:=q
Prof_SAT:=r

"
fitm<-cfa(construct_m, data=df1, missing="ML")
summary(fitm, fit.measures=TRUE, standardized=TRUE)


```

The model remains of good fit: CHi-square/df= 1,52, CFI and TFI above
0.95, RMSEA below 0.6, SRMR\< 0.7.

When we observe the latent variables output, we notice that all
standardised loadings are above 0.6, SAT_3 by little but it is
acceptable. COM and SAT are good measures.

```{r}
semPaths(fitm, what = "path", whatLabels = "std", style = "lisrel",fade=TRUE,
         rotation = 2, layout = "tree3", mar = c(1, 1, 1, 1), 
         nCharNodes = 7,shapeMan = "rectangle", sizeMan = 8, sizeMan2 = 5, 
         curvePivot=TRUE, edge.label.cex = 1, edge.color = "darkblue")
```

The path linking COM and SAT is because we are allowing for the two to
predict each-other.

#### Reliability

```{r}
cronbach(subset(df1, select = c(COM_A1:COM_A4)))$alpha
cronbach(subset(df1, select = c(SAT_1:SAT_3)))$alpha
```

### Introduction of mediators and outcomes + paths

Customer Satisfaction coded as SAT Affective Commitment coded as COM

Repurchase Intention coded as C_REP Cocreation Intention C_CR.

The paths between latent variables ( so the factors, the mediator COM
and SAT, and the outcomes C_REP and C_CR ) are modelled as regression
paths.

`allora questo cancellalo, ma ci sono delle espressioni che ho scritto che sono commented out perchÃ¨ redundant per R. ho fatto riferimento alla slide 28 dell folder 04 delle slide.`

```{r}
construct3 <- "

# MEASUREMENT MODELS
# Factors
Assortment=~Im1+Im2
French_lifestyle=~Im6+Im7
Shop_experience=~Im20+Im21+Im22
Organization=~Im3+Im4+Im5
Luxury=~Im12+Im13
Coolness=~Im17+Im18
Food=~Im10+Im14
Professionalism=~Im16+Im19

# mediators
COM=~COM_A1+COM_A2+COM_A3+COM_A4
SAT=~SAT_1+SAT_2+SAT_3

# Outcomes
C_REP=~ C_REP1+C_REP2+C_REP3
C_CR=~C_CR1+C_CR2+C_CR3+C_CR4


# STRUCTURAL MODELS
# Paths from factors to mediators
COM ~ a*Assortment + b*French_lifestyle + c*Shop_experience + d*Organization + e*Luxury + f*Coolness + g*Food + h*Professionalism

SAT ~ i*Assortment + l*French_lifestyle + m*Shop_experience + n*Organization + o*Luxury + p*Coolness + q*Food + r*Professionalism

# Total effects for mediators
# to COM
A_COM :=a
Fr_COM:=b
Sh_COM:=c
Org_COM:=d
Lux_COM:=e
Cool_COM:=f
Food_COM:=g
Prof_COM:=h

# to SAT
A_SAT:=i
Fr_SAT:=l
Sh_SAT:=m
Org_SAT:=n
Lux_SAT:=o
Cool_SAT:=p
Food_SAT:=q
Prof_SAT:=r

# Paths from mediators to outcomes
#C_REP ~ u*COM+v*SAT 
#C_CR ~ w*COM+z*SAT

# Paths from factors to outcome
C_REP  ~ aa*Assortment + bb*French_lifestyle + cc*Shop_experience + dd*Organization + ee*Luxury + ff*Coolness + gg*Food + hh*Professionalism +u*COM +v*SAT

C_CR ~ ii*Assortment + ll*French_lifestyle + mm*Shop_experience + nn*Organization + oo*Luxury + pp*Coolness + qq*Food + rr*Professionalism + w*COM+ z*SAT

# Indirect paths from factors to outcomes
# passing through COM (edges [a,...,h] and [u,w])
#C_REP ~ ua*Assortment + ub*French_lifestyle + uc*Shop_experience + ud*Organization + ue*Luxury + uf*Coolness + ug*Food + uh*Professionalism

#C_CR ~ wa*Assortment + wb*French_lifestyle + wc*Shop_experience + wd*Organization + we*Luxury + wf*Coolness + wg*Food + wh*Professionalism

# define indirect effects through COM
# to C_REP
ua:=a*u
ub:=b*u
uc:=c*u
ud:=d*u
ue:=e*u
uf:=f*u
ug:=g*u
uh:=h*u

# to C_CR
wa:=a*w
wb:=b*w
wc:=c*w
wd:=d*w
we:=e*w
wf:=f*w
wg:=g*w
wh:=h*w

# passing through SAT (edges [i,...,r] and [v,z])
#C_REP ~ vi*Assortment + vl*French_lifestyle + vm*Shop_experience + vn*Organization + vo*Luxury + vp*Coolness + vq*Food + vr*Professionalism

#C_CR ~ zi*Assortment + zl*French_lifestyle + zm*Shop_experience + zn*Organization + zo*Luxury + zp*Coolness + zq*Food + zr*Professionalism

# define indirect effects 
# through COM
vi:=i*v
vl:=l*v
vm:=m*v
vn:=n*v
vo:=o*v
vp:=p*v
vq:=q*v
vr:=r*v

# Through SAT
zi:=i*z
zl:=l*z
zm:=m*z
zn:=n*z
zo:=o*z
zp:=p*z
zq:=q*z
zr:=r*z

# Total effects, from factors to outcomes
# to C_REP
A_REP := aa+ua+vi
Fr_REP:=bb+ub+vl
Sh_REP:=cc+uc+vm
Org_REP:=dd+ud+vn
Lux_REP:=ee+ue+vo
Cool_REP:=ff+uf+vp
Food_REP:=gg+ug+vq
Prof_REP:=hh+uh+vr

# to C_CR
A_CR:=ii+wa+zi
Fr_CR:=ll+wb+zl
Sh_CR:=mm+wc+zm
Org_CR:=nn+wd+zn
Lux_CR:=oo+we+zo
Cool_CR:=pp+wf+zp
Food_CR:=qq+wg+zq
Prof_CR:=rr+wh+zr

"

fit3<-cfa(construct3, data=df1, missing="ML")
summary(fit3, fit.measures=TRUE, standardized=TRUE)


```

-   Chi-Square/D=1.64

-   CFI and TFI above 0.95

-   RMSEA measures the degree of misspecification of the model, it
    measures 0.034, which is below the 0.05 indicating that our model is
    a close fit to the true model.

-    acceptability threshold

-   SRMR 0.041

All latent variables have std.all above 0.6

```{r}
lavaanPlot(name="plot", fit3, labels = NULL)
```

`allora poi cancella anche quest, ma come vedi questo grafico con semapth non riesce a mettere i emdiatori al posto giusto, ora if you change the layout with tree,tre2,tree3 vedi che cambia ma non riesco a metterli in mezzo. pero lavaanplot va benissimo e quindi volevo si puo anche togliere`

```{r}
semPaths(fit3,what = "path", whatLabels = "std", style = "lisrel",exoCov = T,
         rotation = 2, layout = "tree", mar = c(1, 2, 1, 2), 
         nCharNodes = 7,shapeMan = "rectangle", sizeMan = 7, sizeMan2 = 5, 
         curvePivot=TRUE, edge.label.cex = 1.2, edge.color = "darkblue")


semPaths(fit3, what="std", fade=F, edge.label.cex=1, sizeMan=7, sizeLat=7, exoCov = F, exoVar = F, intercepts = F, rotation = 2)

semPaths(fit3, what="path", whatLabels="std")
```

```{r}
construct_trial <- "

# MEASUREMENT MODELS
# Factors
Assortment=~Im1+Im2
French_lifestyle=~Im6+Im7
Shop_experience=~Im20+Im21+Im22
Organization=~Im3+Im4+Im5
Luxury=~Im12+Im13
Coolness=~Im17+Im18
Food=~Im10+Im14
Professionalism=~Im16+Im19

# mediators
COM=~COM_A1+COM_A2+COM_A3+COM_A4
SAT=~SAT_1+SAT_2+SAT_3

# Outcomes
C_REP=~ C_REP1+C_REP2+C_REP3
C_CR=~ C_CR1+C_CR2+C_CR3+C_CR4

# STRUCTURAL MODELS
# latent factors(exogenous) to latent mediator (endogenuous)
COM ~ Assortment + French_lifestyle + Shop_experience + Organization + Luxury + Coolness + Food + Professionalism

SAT ~ Assortment + French_lifestyle + Shop_experience + Organization + Luxury + Coolness + Food + Professionalism

#latent factors (exogenous) to latent outcome factors (endogenous)
C_REP  ~ Assortment + French_lifestyle + Shop_experience + Organization + Luxury + Coolness + Food + Professionalism +COM +SAT

C_CR ~ Assortment + French_lifestyle + Shop_experience + Organization + Luxury + Coolness + Food + Professionalism + COM + SAT"



fit_trial<- sem(construct_trial, data=df1)
summary(fit_trial,fit.measures=TRUE, standardized=TRUE)


```

#### 2.Are the mechanism driving satisfaction and affective commitment similar? Are satisfaction and affective commitment mediating the impact of image perceptions on outcomes? If yes for which outcomes?

#### 3. What is driving the two distinct outcomes (repurchase and co-creation intention): Please rank the image dimensions with respect to the total effect on each outcome? Interpret your results

References:

[Confirmatory Factor Analysis (CFA) in R with lavaan
(ucla.edu)](https://stats.oarc.ucla.edu/r/seminars/rcfa/#s3b)
